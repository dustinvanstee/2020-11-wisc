{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T00:52:47.878600Z",
     "start_time": "2020-11-21T00:52:47.866593Z"
    },
    "tags": [
     "import"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA available: True  Using device 1\n",
      "Mon Nov 23 13:08:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    39W / 300W |      2MiB / 32510MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    42W / 300W |      2MiB / 32510MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   37C    P0    39W / 300W |      2MiB / 32510MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   59C    P0   274W / 300W |  14911MiB / 32510MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    3   N/A  N/A     70094      C   ...ce-pytorch-1.6/bin/python    14909MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pdb\n",
    "import time\n",
    "import argparse\n",
    "import dask\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "PROJ_ROOT=\"/gpfs/home/s4s004/vanstee/2020-11-wisc\"\n",
    "sys.path.append(os.path.abspath(PROJ_ROOT + \"/pytorch-retinanet\"))\n",
    "\n",
    "import retinanet.model as model\n",
    "\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "\tUnNormalizer, Normalizer\n",
    "\n",
    "assert torch.__version__.split('.')[0] == '1'\n",
    "\n",
    "def nprint(mystring) :\n",
    "    print(\"**{}** : {}\".format(sys._getframe(1).f_code.co_name,mystring))\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "# utility print function\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "print('CUDA available: {}  Using device {}'.format(torch.cuda.is_available(), os.environ['CUDA_VISIBLE_DEVICES']))\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T00:53:22.649277Z",
     "start_time": "2020-11-21T00:53:22.640764Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_config(cfg_in={}):\n",
    "    config = {}\n",
    "    # Tx = 10          # variable per batch sequence length\n",
    "    config[\"model\"] = None\n",
    "    config[\"video_file\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreXVID.avi\"  # \n",
    "    config[\"output_directory\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/\"  # \n",
    "    config[\"total_frames\"] = 10  # \n",
    "    config[\"sample_rate\"] = 1  # not implemented yet\n",
    "\n",
    "    # overwrite configs if passed\n",
    "    for (k,v) in cfg_in.items() :\n",
    "        if(k in config.keys()) :\n",
    "            npt(\"Overriding Config {}:{} with {}\".format(k,config[k],v))\n",
    "        else :\n",
    "            npt(\"Warning, adding a new unknown config {}:{}\".format(k,v))\n",
    "\n",
    "    # inplace update\n",
    "    config.update(cfg_in)\n",
    "    # npt(\"{}\".format(config))\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T00:53:23.397914Z",
     "start_time": "2020-11-21T00:53:23.394310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**<module>** : {'model': None, 'video_file': '/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreXVID.avi', 'output_directory': '/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/', 'total_frames': 10, 'sample_rate': 1}\n"
     ]
    }
   ],
   "source": [
    "cfg = get_config()\n",
    "nprint(\"{}\".format(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T19:48:54.425259Z",
     "start_time": "2020-11-18T19:48:54.422866Z"
    }
   },
   "source": [
    "# split_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T00:53:24.801393Z",
     "start_time": "2020-11-21T00:53:24.787575Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_video(video_file, output_directory, frame_limit=5, sample_rate=1) :\n",
    "\n",
    "    cap  = cv2.VideoCapture(video_file)\n",
    "    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) # fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    secs = total_frames / fps\n",
    "    nprint(\"Total number of frames  = {} (frames)\".format(total_frames))\n",
    "    nprint(\"Frame rate              = {} (fps)\".format(fps))\n",
    "    nprint(\"Total seconds for video = {} (s)\".format(secs))\n",
    "    \n",
    "    \n",
    "    if(frame_limit > total_frames) :\n",
    "        frame_limit = int(total_frames)\n",
    "    sample_rate = 1\n",
    "    framecnt = 1 # equals one b/c I read a frame ...\n",
    "    nprint(\"Total Frames to annotate= {}\".format(int(frame_limit/sample_rate)))\n",
    "    \n",
    "    labels_string = \"\"\n",
    "    for i in range(frame_limit) :\n",
    "        if(i % 1 == 0 ):\n",
    "            nprint(\"Loaded {} frames\".format(i))\n",
    "        ret, frame = cap.read()\n",
    "        output_fn = output_directory + \"{:0004d}\".format(i)+\"_stitch.jpg\"\n",
    "        cv2.imwrite(output_fn, frame )\n",
    "        labels_string += \"{},,,,,\\n\".format(output_fn)\n",
    "    \n",
    "    cap.release()\n",
    "    #print(labels_string)\n",
    "    f = open(output_directory + \"/labels.csv\", 'w')\n",
    "    f.write(labels_string)\n",
    "    nprint(\"Wrote labels to {}\".format(output_directory + \"/labels.csv\"))\n",
    "    nprint(\"Processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaks up video into a directory with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:55:34.468760Z",
     "start_time": "2020-11-20T19:47:57.843728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**split_video** : Total number of frames  = 7500.0 (frames)\n",
      "**split_video** : Frame rate              = 25.0 (fps)\n",
      "**split_video** : Total seconds for video = 300.0 (s)\n",
      "**split_video** : Total Frames to annotate= 10\n",
      "**split_video** : Loaded 0 frames\n",
      "**split_video** : Loaded 1 frames\n",
      "**split_video** : Loaded 2 frames\n",
      "**split_video** : Loaded 3 frames\n",
      "**split_video** : Loaded 4 frames\n",
      "**split_video** : Loaded 5 frames\n",
      "**split_video** : Loaded 6 frames\n",
      "**split_video** : Loaded 7 frames\n",
      "**split_video** : Loaded 8 frames\n",
      "**split_video** : Loaded 9 frames\n",
      "**split_video** : Wrote labels to /gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre//labels.csv\n",
      "**split_video** : Processing complete\n"
     ]
    }
   ],
   "source": [
    "split_video(cfg[\"video_file\"], cfg[\"output_directory\"], frame_limit=cfg[\"total_frames\"], sample_rate=cfg[\"sample_rate\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/\n",
      "/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/\n",
      "   7502    7502  120022\n"
     ]
    }
   ],
   "source": [
    "print(cfg[\"output_directory\"])\n",
    "print(cfg[\"output_directory\"])\n",
    "!ls {cfg[\"output_directory\"]} | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T01:34:49.598081Z",
     "start_time": "2020-11-21T01:34:49.590698Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_caption(image, box, caption):\n",
    "\n",
    "    b = np.array(box).astype(int)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 0), 2)\n",
    "    cv2.putText(image, caption, (b[0], b[1] - 10), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T14:55:31.008807Z",
     "start_time": "2020-11-21T14:55:30.977527Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def annotate_video(train_file, class_list, model, output_directory, display_image=False) :\n",
    "\n",
    "    dataset_val = CSVDataset(train_file=train_file, class_list=class_list, \n",
    "                             transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "    \n",
    "    sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False,)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=100, collate_fn=collater) # , batch_sampler=sampler_val\n",
    "    \n",
    "    retinanet = torch.load(model)\n",
    "    \n",
    "    use_gpu = True\n",
    "    \n",
    "    if use_gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            retinanet = retinanet.cuda()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "    else:\n",
    "        retinanet = torch.nn.DataParallel(retinanet)\n",
    "    \n",
    "    retinanet.eval()\n",
    "    unnormalize = UnNormalizer()\n",
    "    for idx, data in enumerate(dataloader_val):\n",
    "        output_filename = output_directory + \"/annotated_\" + \"{:04d}\".format(idx)+\".jpg\"\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            st = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                scores, classification, transformed_anchors = retinanet(data['img'].cuda().float())\n",
    "            else:\n",
    "                scores, classification, transformed_anchors = retinanet(data['img'].float())\n",
    "            print('Elapsed time: {}'.format(time.time()-st))\n",
    "            idxs = np.where(scores.cpu()>0.5)\n",
    "            img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
    "    \n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "    \n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "            img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "            nprint(\"Detected {} boxes\".format(idxs[0].shape[0]))\n",
    "            for j in range(idxs[0].shape[0]):\n",
    "                bbox = transformed_anchors[idxs[0][j], :]\n",
    "                x1 = int(bbox[0])\n",
    "                y1 = int(bbox[1])\n",
    "                x2 = int(bbox[2])\n",
    "                y2 = int(bbox[3])\n",
    "                label_name = dataset_val.labels[int(classification[idxs[0][j]])]\n",
    "                draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "                \n",
    "    \n",
    "            # cv2.imshow('img', img)\n",
    "            # cv2.waitKey(0)\n",
    "            if(display_image) :\n",
    "                plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "                plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "                plt.show()\n",
    "            nprint(\"Writing File {}\".format(output_filename))\n",
    "            cv2.imwrite(output_filename, img )\n",
    "    \n",
    "    \n",
    "    #annotate_images(retinanet, dataset_val, dataloader_val, output_directory, display_image=False)\n",
    "\n",
    "\n",
    "# train_file = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/SocialDistance.convert/objdet/labels.csv\"\n",
    "# class_list = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/SocialDistance.convert/objdet/lookup.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Town Centre Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-21T14:59:12.238Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add to project config\n",
    "cfg[\"train_file\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/labels.csv\"\n",
    "cfg[\"class_list\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/lookup.csv\"\n",
    "#cfg[\"model_dir\"]  = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/training_results/model_final.pt\"\n",
    "cfg[\"model_dir\"]  = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/csv_retinanet_45.pt\"\n",
    "cfg[\"model\"]      = torch.load(cfg[\"model_dir\"])\n",
    "cfg[\"annot_dir\"]  = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreAnnotated/\"\n",
    "\n",
    "annotate_video(cfg[\"train_file\"], cfg[\"class_list\"], cfg[\"model_dir\"], cfg[\"annot_dir\"], display_image=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Video with Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T14:56:55.248233Z",
     "start_time": "2020-11-21T14:56:55.238151Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def assemble_video(output_directory, output_filename, debug=False) :\n",
    "    output_directory\n",
    "    #out = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
    "    fps=25\n",
    "    h=608\n",
    "    w=1056\n",
    "    \n",
    "    output_filename = output_directory + output_filename\n",
    "    print(\"Writing output video to {}\".format(output_filename))\n",
    "    out  = cv2.VideoWriter(output_filename, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w,h), True)\n",
    "\n",
    "    filelist =  glob.glob(output_directory + '/*.jpg')\n",
    "    for i in range(len(filelist)) :\n",
    "        file = output_directory + \"annotated_{:04d}.jpg\".format(i)\n",
    "        if debug : print(file)\n",
    "        img = cv2.imread(file)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        if debug : print(size)\n",
    "        out.write(img)\n",
    "        #img_array.append(img)\n",
    "    out.release()\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T14:57:06.043780Z",
     "start_time": "2020-11-21T14:56:55.935059Z"
    }
   },
   "outputs": [],
   "source": [
    "output_directory = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreAnnotated/\"\n",
    "assemble_video(output_directory, \"example.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid Social Distancing Example\n",
    "\n",
    "General algorithm, \n",
    "1. for each detected person box, calculate foot pixel position\n",
    "2. then use homographic projection to map to real world coordinates\n",
    "3. then use simple KD tree to determine collision\n",
    "4. render collisions a different color "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:23:32.965010Z",
     "start_time": "2020-11-19T18:23:32.956596Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  # import the numpy library\n",
    "\t\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "# the following two parallel arrays allow us to map from a source of image coordinates in a tilted plane\n",
    "# to a target set of corrdinates in a destination frame\n",
    "# use a proam such as ImageMouseClick.py\n",
    "# to create a list of coordinates for points where you know or can calulate the world coordinates\n",
    "# for example image coordinates [143, 468] was found to be associated with  world coordinate of 10',25'\n",
    "\n",
    "pts_src = np.array([[143, 468], [789,178], [541,544], [1194, 671], [1083,214], [1560, 273]])\n",
    "pts_dst = np.array([[10,25],    [10,65],   [24.7,25], [44.7,25],   [24.7, 65], [44.7,65]])\n",
    "\n",
    "# calculate matrix H. DO this once for an image to compute H for camera setup, \n",
    "# resuse H over and over for rest of images\n",
    "\n",
    "homograpy, status  = cv2.findHomography(pts_src, pts_dst)\n",
    "homograpy_inv, status = cv2.findHomography(pts_dst, pts_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:23:33.502551Z",
     "start_time": "2020-11-19T18:23:33.493982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[10.013301, 24.920633],\n",
       "        [10.161524, 65.04503 ],\n",
       "        [24.677074, 25.029789],\n",
       "        [44.709366, 25.050034],\n",
       "        [24.425364, 65.02927 ],\n",
       "        [44.813377, 64.925255]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify .. Convert Px space to World Coordinates\n",
    "# footPositionPx = np.array([[[1250,250], [989,893]]], dtype='float32')\n",
    "footPositionPx = np.array([[[143, 468], [789,178], [541,544], [1194, 671], [1083,214], [1560, 273]]], dtype='float32')\n",
    "print(footPositionPx.shape)\n",
    "FootPostionWorldWc = cv2.perspectiveTransform(footPositionPx, homograpy)\n",
    "FootPostionWorldWc\n",
    "\n",
    "# FootPostionWorldWc = cv2.perspectiveTransform(footPositionPx, h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:28:42.566976Z",
     "start_time": "2020-11-19T18:28:42.542214Z"
    }
   },
   "outputs": [],
   "source": [
    "#x1,y1,x2,y2 (upper left -> lower right)\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def calc_foot_position(bbox) : \n",
    "        x1 = int(bbox[0])\n",
    "        y1 = int(bbox[1])\n",
    "        x2 = int(bbox[2])\n",
    "        y2 = int(bbox[3])    \n",
    "        return (int((x1+x2)/2), int(y2))\n",
    "\n",
    "def get_pairs(point_list_px,dist=6) :\n",
    "    #print(point_list_px)\n",
    "    # Transform to real world coordinates\n",
    "    point_list_px = np.expand_dims(point_list_px, axis=0)\n",
    "    point_list_px = point_list_px.astype('float32')\n",
    "    \n",
    "    #print(point_list_px.shape)\n",
    "    point_list_wc = cv2.perspectiveTransform(point_list_px, homograpy)\n",
    "    point_list_wc = point_list_wc.squeeze(0)\n",
    "    # T = KDTree(FootPostionWorldXY[0,:])\n",
    "    tree = KDTree(point_list_wc)\n",
    "    pairs = list(tree.query_pairs(r=dist))\n",
    "    #print(\"Found pairs {}\".format(pairs))\n",
    "    # Create a simple dictionary to highlight index that has collsions\n",
    "    box_collision_dict  = {}\n",
    "    for p in pairs :\n",
    "        box_collision_dict[p[0]] = 1\n",
    "        box_collision_dict[p[1]] = 1\n",
    "    #print(box_collision_dict)\n",
    "    return box_collision_dict\n",
    "\n",
    "def draw_covid_boxes(img, idxs,classification, transformed_anchors, label_names) :\n",
    "    \n",
    "    foot_position_px_list = []\n",
    "    for j in range(idxs[0].shape[0]):\n",
    "        bbox = transformed_anchors[idxs[0][j], :]\n",
    "        foot_position_px_list.append(calc_foot_position(bbox))\n",
    "    \n",
    "    box_collision_dict = get_pairs(np.array(foot_position_px_list))\n",
    "\n",
    "    for j in range(idxs[0].shape[0]):\n",
    "        bbox = transformed_anchors[idxs[0][j], :]\n",
    "        x1 = int(bbox[0])\n",
    "        y1 = int(bbox[1])\n",
    "        x2 = int(bbox[2])\n",
    "        y2 = int(bbox[3])\n",
    "        foot_position_px = calc_foot_position(bbox)\n",
    "        \n",
    "        label_name = label_names[int(classification[idxs[0][j]])]\n",
    "        draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "        #print(j, box_collision_dict.keys())\n",
    "        if( j in box_collision_dict.keys()) :\n",
    "            # collision\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2) #BGR !\n",
    "        else :\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # Create an overlay for a warped circle ..\n",
    "        \n",
    "        overlay = np.zeros((img.shape[0],img.shape[1]), np.uint8)\n",
    "        cv2.circle(overlay, foot_position_px, 50, 255)\n",
    "        cp = np.transpose(np.where(img==255))\n",
    "        #cv2.addWeighted(overlay, 0.75, output, 1 - alpha, 0, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:28:43.058522Z",
     "start_time": "2020-11-19T18:28:43.053054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1, 1: 1, 2: 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "point_list = np.array([[1,2],[1,1],[10,10]])\n",
    "get_pairs(point_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:28:43.653376Z",
     "start_time": "2020-11-19T18:28:43.644204Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_simple_boxes(img, idxs,classification, transformed_anchors, label_names) :\n",
    "    for j in range(idxs[0].shape[0]):\n",
    "        bbox = transformed_anchors[idxs[0][j], :]\n",
    "        #print(bbox)\n",
    "        x1 = int(bbox[0])\n",
    "        y1 = int(bbox[1])\n",
    "        x2 = int(bbox[2])\n",
    "        y2 = int(bbox[3])\n",
    "        label_name = label_names[int(classification[idxs[0][j]])]\n",
    "        draw_caption(img, (x1, y1, x2, y2), label_name)\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:28:44.255616Z",
     "start_time": "2020-11-19T18:28:44.231217Z"
    }
   },
   "outputs": [],
   "source": [
    "def annotate_video_covid(cfg, display_image=False, debug=False) :\n",
    "\n",
    "    dataset_val = CSVDataset(train_file=cfg[\"train_file\"], class_list=cfg[\"class_list\"], \n",
    "                             transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "    \n",
    "    # sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False,)\n",
    "    dataloader_val = DataLoader(dataset_val, num_workers=100, collate_fn=collater, batch_size=1) # , batch_sampler=sampler_val\n",
    "    \n",
    "    retinanet = torch.load(cfg[\"model_dir\"])\n",
    "    \n",
    "    use_gpu = True\n",
    "    \n",
    "    if use_gpu:\n",
    "        if torch.cuda.is_available():\n",
    "            retinanet = retinanet.cuda()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "    else:\n",
    "        retinanet = torch.nn.DataParallel(retinanet)\n",
    "    \n",
    "    retinanet.eval()\n",
    "    unnormalize = UnNormalizer()\n",
    "    for idx, data in enumerate(dataloader_val):\n",
    "        output_filename = cfg[\"annot_dir\"] + \"/annotated_\" + \"{:04d}\".format(idx)+\".jpg\"\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            st = time.time()\n",
    "            if torch.cuda.is_available():\n",
    "                scores, classification, transformed_anchors = retinanet(data['img'].cuda().float())\n",
    "            else:\n",
    "                scores, classification, transformed_anchors = retinanet(data['img'].float())\n",
    "            if debug : print('Elapsed time: {}'.format(time.time()-st))\n",
    "            idxs = np.where(scores.cpu()>0.5)\n",
    "            img = np.array(255 * unnormalize(data['img'][0, :, :, :])).copy()\n",
    "    \n",
    "            img[img<0] = 0\n",
    "            img[img>255] = 255\n",
    "    \n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "            img = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "            if debug : nprint(\"Detected {} boxes\".format(idxs[0].shape[0]))            \n",
    "            #draw_simple_boxes(img, idxs,classification, transformed_anchors, dataset_val.labels)\n",
    "            draw_covid_boxes(img, idxs,classification, transformed_anchors, dataset_val.labels)\n",
    "    \n",
    "            # cv2.imshow('img', img)\n",
    "            # cv2.waitKey(0)\n",
    "            if(display_image) :\n",
    "                plt.figure(figsize = (15,15))\n",
    "                plt.imshow(img, interpolation = 'bicubic')\n",
    "                plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "                plt.show()\n",
    "            if debug : nprint(\"Writing File {}\".format(output_filename))\n",
    "            cv2.imwrite(output_filename, img )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:30:02.890200Z",
     "start_time": "2020-11-19T18:28:44.967590Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-efab6f657e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"annot_dir\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m\"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreCovid/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mannotate_video_covid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_image\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-6075acdef925>\u001b[0m in \u001b[0;36mannotate_video_covid\u001b[0;34m(cfg, display_image, debug)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdebug\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mnprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detected {} boxes\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#draw_simple_boxes(img, idxs,classification, transformed_anchors, dataset_val.labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mdraw_covid_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# cv2.imshow('img', img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-5a07f1f6acbc>\u001b[0m in \u001b[0;36mdraw_covid_boxes\u001b[0;34m(img, idxs, classification, transformed_anchors, label_names)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moverlay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverlay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfoot_position_px\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m#cv2.addWeighted(overlay, 0.75, output, 1 - alpha, 0, output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg[\"train_file\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/labels.csv\"\n",
    "cfg[\"class_list\"] = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentre/lookup.csv\"\n",
    "cfg[\"model_dir\"]  = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/training_results/model_final.pt\"\n",
    "cfg[\"model\"]      = torch.load(cfg[\"model_dir\"])\n",
    "cfg[\"annot_dir\"]  = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreCovid/\"\n",
    "annotate_video_covid(cfg, display_image=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:06:46.582939Z",
     "start_time": "2020-11-19T16:06:45.145371Z"
    }
   },
   "source": [
    "# Assemble Video for Social Distancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T18:30:17.167149Z",
     "start_time": "2020-11-19T18:30:07.974429Z"
    }
   },
   "outputs": [],
   "source": [
    "output_directory = \"/gpfs/home/s4s004/vanstee/2020-11-wisc/inference/TownCentreCovid/\"\n",
    "assemble_video(output_directory, \"example.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize in world coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combo(onefile, rnd_base = 0):\n",
    "    # homagraphy transform of image points\n",
    "    \n",
    "    repeatfile = onefile\n",
    "\n",
    "    imageIn = np.zeros([1,len(repeatfile),2], np.float32)\n",
    "    imageIn[0,:,0] = (repeatfile.xmin + repeatfile.xmax)/2\n",
    "    imageIn[0,:,1] = repeatfile.ymax\n",
    "    #imageIn = list(zip(onefile.xmin, onefile.ymin))\n",
    "    pointsOut = cv2.perspectiveTransform(imageIn, h)\n",
    "\n",
    "    streetLineDots = [[11, 884], [1086, 214], [39, 910], \n",
    "                   [1076, 243], [914, 905], [1511, 281], \n",
    "                   [975, 915], [1488, 345], \n",
    "                   [978, 911], [1482, 355]]\n",
    "    streetLineDotsIn = np.zeros([1,len(streetLineDots),2], np.float32)\n",
    "    streetLineDotsIn[0,:] = streetLineDots\n",
    "    streetLineDotsOut = cv2.perspectiveTransform(streetLineDotsIn, h)\n",
    "\n",
    "    fig  = plt.subplots(figsize=(18,9))\n",
    "    gs = gridspec.GridSpec(1, 2, width_ratios=[1,4]) \n",
    "\n",
    "    # plot chart on left  ***************************************************************************\n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ylmin = 0 #int(min(pointsOut[0,:,1]))\n",
    "    ylmax = 100 # 1 + int(max(pointsOut[0,:,1]+1))\n",
    "    \n",
    "    # streets *******************************************\n",
    "    rect = plt.Rectangle((26, ylmin), 20, ylmax, color='k', alpha=0.1)\n",
    "    ax0.add_patch(rect)\n",
    "    rect = plt.Rectangle((27, ylmin), 18, ylmax, color='w', alpha=0.4)\n",
    "    ax0.add_patch(rect)\n",
    "    \n",
    "    # sidewalk *******************************************\n",
    "    rect = plt.Rectangle((10, 66), 16, 9, color='k', alpha=0.1)\n",
    "    ax0.add_patch(rect)\n",
    "    \n",
    "    \n",
    "    # ******************************************************\n",
    "    # plot posiotn on strip chart to left - green\n",
    "    # plot center blue point\n",
    "    if rnd_base == 0 :\n",
    "        xrnd = pointsOut[0,:,0]\n",
    "        yrnd = pointsOut[0,:,1]\n",
    "    else:\n",
    "        xrnd = rnd_base * np.round(pointsOut[0,:,0]/rnd_base)\n",
    "        yrnd = rnd_base * np.round(pointsOut[0,:,1]/rnd_base)\n",
    "    plt.scatter(xrnd, yrnd, alpha = 1)\n",
    "    # plot green bubble\n",
    "    plt.scatter(xrnd, yrnd, s=720, facecolors='none', edgecolors='lime', linewidths = 3)\n",
    "    #plt.scatter(streetLineDotsOut[0,:,0], streetLineDotsOut[0,:,1], c='k')\n",
    "    plt.xticks(rotation = -90)\n",
    "    ax0.set_xticks(np.arange(0,60,3))\n",
    "    ax0.set_yticks(np.arange(0, 100,3))\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xlim(0, 60)\n",
    "    plt.grid(c='cyan')\n",
    "\n",
    "    pers = np.zeros([1,len(repeatfile),2], np.float32)\n",
    "    pers[0,:,0] = (repeatfile.xmin + repeatfile.xmax)/2\n",
    "    pers[0,:,1] = repeatfile.ymax\n",
    "    #a = list(zip(onefile.xmin, onefile.ymin))\n",
    "    pointsOut = cv2.perspectiveTransform(pers, h)\n",
    "\n",
    "    T = KDTree(pointsOut[0,:])\n",
    "    #test = pointsOut[0,0]\n",
    "    #idx = T.query_ball_point(test,r=10)\n",
    "    #pointsOut[0,idx]\n",
    "    # ***********************************\n",
    "    # ************************************\n",
    "    \n",
    "    R = 7\n",
    "    \n",
    "    \n",
    "    # ***********************************\n",
    "    # ************************************\n",
    "    # plot posiotn on strip chart to left, pair of people  - red\n",
    "    pairs = T.query_pairs(r=R)\n",
    "    idx = 0\n",
    "    d = Dmetric(pointsOut, T.query_pairs(R))\n",
    "    for pair in pairs:\n",
    "        #left pair\n",
    "        if rnd_base == 0 :\n",
    "            xrnd = pointsOut[0,pair[0],0]\n",
    "            yrnd = pointsOut[0,pair[0],1]\n",
    "        else:\n",
    "            xrnd = rnd_base * np.round(pointsOut[0,pair[0],0]/rnd_base)\n",
    "            yrnd = rnd_base * np.round(pointsOut[0,pair[0],1]/rnd_base) \n",
    "        plt.scatter(xrnd, yrnd, s=720, facecolors='none', edgecolors='r', linewidths = 3)\n",
    "        #right pair\n",
    "        if rnd_base == 0 :\n",
    "            xrnd = pointsOut[0,pair[1],0]\n",
    "            yrnd = pointsOut[0,pair[1],1]\n",
    "        else:\n",
    "            xrnd = rnd_base * np.round(pointsOut[0,pair[1],0]/rnd_base)\n",
    "            yrnd = rnd_base * np.round(pointsOut[0,pair[1],1]/rnd_base)        \n",
    "        #xrnd = base * np.round(pointsOut[0,pair[1],0]/base) \n",
    "        #yrnd = base * np.round(pointsOut[0,pair[1],1]/base)    \n",
    "        plt.scatter(xrnd, yrnd, s=720, facecolors='none', edgecolors='r', linewidths = 3) \n",
    "        #ax0.text(46, 93, 'Distance\\nAlert', fontsize=15,\n",
    "            #bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n",
    "        #d = Dmetric(pointsOut, T.query_pairs(R))\n",
    "        #plt.text(pointsOut[0,pair[0],0], pointsOut[0,pair[0],1] , '{:5.1f}'.format(d[idx]), color = 'red', fontsize=15)\n",
    "        idx += 1\n",
    "\n",
    "\n",
    "    # plot Image on right *******************************************************************\n",
    "    # ***************************************************************************************\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    img_fn = './Frames/{}'.format(onefile.iloc[0].image)\n",
    "    img = cv2.imread(img_fn)\n",
    "    img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img2, interpolation='nearest')\n",
    "    # green rectangles\n",
    "    for index, person in onefile.iterrows():\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((person['xmin'],person['ymin']), \n",
    "                                 person['width'],person['ymax']-person['ymin'],\n",
    "                                 linewidth=1,\n",
    "                                 edgecolor='lime',\n",
    "                                 facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax1.add_patch(rect)\n",
    "    # RED rectangles for pairs\n",
    "    T = KDTree(pointsOut[0,:])\n",
    "    pairs = T.query_pairs(r=R)\n",
    "    idx = 0\n",
    "    d = Dmetric(pointsOut, T.query_pairs(R))\n",
    "    for pair in pairs:\n",
    "        one = repeatfile.iloc[pair[0]]\n",
    "        widthOne = one.xmax - one.xmin\n",
    "        heightOne = one.ymax - one.ymin\n",
    "        xminsOne = one.xmin\n",
    "        yminsOne = one.ymin\n",
    "        rect = patches.Rectangle((xminsOne, yminsOne), \n",
    "                                 widthOne, heightOne,\n",
    "                                 linewidth= 3,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax1.add_patch(rect)\n",
    "\n",
    "        two = repeatfile.iloc[pair[1]]\n",
    "        widthTwo = two.xmax - two.xmin\n",
    "        heightTwo = two.ymax - two.ymin\n",
    "        xminsTwo = two.xmin\n",
    "        yminsTwo = two.ymin\n",
    "        rect = patches.Rectangle((xminsTwo, yminsTwo), \n",
    "                                 widthTwo, heightTwo,\n",
    "                                 linewidth= 3,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        # Add the patch to the Axes\n",
    "        ax1.add_patch(rect)    \n",
    "        ax1.text(20, 80, 'Distance\\nAlert < 6 feet', fontsize=15,\n",
    "            bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n",
    "        #d = Dmetric(pointsOut, T.query_pairs(R))\n",
    "        ####plt.text(pointsOut[0,pair[0],0], pointsOut[0,pair[0],1], '{:5.1f}'.format(d[idx]), color = 'red', fontsize=15)\n",
    "        #plt.text(onefile.iloc[pair[0]].xmax, onefile.iloc[pair[0]].ymax , '{:5.1f}'.format(d[idx]) , color = 'red', fontsize=15)\n",
    "        idx += 1\n",
    "    plt.tight_layout()  \n",
    "    newOutput = '{:04d}'.format(int(onefile.iloc[0,:].image.split('-')[-1].split('.')[0]))\n",
    "    plt.savefig('./Alerts/alert-{}'.format(newOutput))\n",
    "    plt.clf()\n",
    "    #plt.close()\n",
    "    #plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "persons_df = pd.read_csv('persons_df.csv')\n",
    "#for index, ones in persons_df.iterrows():\n",
    "personsList = sorted(persons_df.image.unique())\n",
    "#personsList = ['out-0005.jpg']\n",
    "i = 0\n",
    "#lastOne = persons_df[persons_df.image == 'out-0001.jpg']\n",
    "for fn in personsList:\n",
    "    onefile = persons_df[persons_df.image == fn]\n",
    "    plot_combo(onefile)  \n",
    "    if (i % 1) == 0:\n",
    "        print (i,' out of 7500')\n",
    "    i += 1\n",
    "    plt.clf()\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:powerai-fastai]",
   "language": "python",
   "name": "conda-env-powerai-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
